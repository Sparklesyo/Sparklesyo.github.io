<!DOCTYPE html><html lang="cn"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Towards Playing Full MOBA Games with Deep Reinforcement Learning | Sparkle</title><script src="https://cdn.bootcss.com/valine/1.4.4/Valine.min.js"></script><link rel="stylesheet" href="/css/arknights.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css"><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 5.4.0"></head><body><header><nav><a href="/">Home</a><a href="/archives/">Archives</a></nav></header><main style="flex-direction: row-reverse"><article><div id="post-bg"><div id="post-title"><h1>Towards Playing Full MOBA Games with Deep Reinforcement Learning</h1><hr></div><div id="post-content"><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>Artificial Intelligence for games, a.k.a. Game AI, has been actively studied for decades. We have witnessed the success of AI agents in many game types, including board games like Go [30], Atari series [21], first-person shooting (FPS) games like Capture the Flag [15], video games like Super<br>Smash Bros [6], card games like Poker [3], etc. Nowadays, sophisticated strategy video games attract attention as they capture the nature of the real world [2], e.g., in 2019, AlphaStar achieved the grandmaster level in playing the general real-time strategy (RTS) game - StarCraft 2 [33].</p>
<p>As a sub-genre of RTS games, Multi-player Online Battle Arena (MOBA) has also attracted much attention recently [38,36,2]. Due to its playing mechanics which involve multi-agent competition and cooperation, imperfect information, complex action control, and enormous state-action space, MOBA is considered as a preferable testbed for AI research [29,25]. Typical MOBA games include Honor of Kings, Dota, and League of Legends. In terms of complexity, a MOBA game, such as Honor of Kings, even with significant discretization, could have a state and action space of magnitude 1020000[36], while that of a conventional Game AI testbed, such as Go, is at most10360[30]. MOBA games are further complicated by the real-time strategies of multiple heroes (each hero is uniquely designed to have diverse playing mechanics), particularly in the 5 versus 5 (5v5) mode where two teams (each with 5 heroes selected from the hero pool) compete against each other1.</p>
<p>In spite of its suitability for AI research, mastering the playing of MOBA remains to be a grand challenge for current AI systems. State-of-the-art work for MOBA 5v5 game is OpenAI Five for playing Dota 2 [2]. It trains with self-play reinforcement learning (RL). However, OpenAI Five plays with one major limitation2, i.e., only 17 heroes were supported, despite the fact that the hero-varying and team-varying playing mechanism is the soul of MOBA [38, 29].</p>
<p>As the most fundamental step towards playing full MOBA games, scaling up the hero pool is challenging for self-play reinforcement learning, because the number of agent combinations, i.e.,lineups, grows polynomially with the hero pool size. The agent combinations are 4,900,896 (C1017×C510) for 17 heroes, while exploding to 213,610,453,056 (C1040× C510) for 40 heroes. Considering the fact that each MOBA hero is unique and has a learning curve even for experienced human players,existing methods by randomly presenting these disordered hero combinations to a learning system can lead to “learning collapse” [1], which has been observed from both OpenAI Five [2] and our experiments. For instance, OpenAI attempted to expand the hero pool up to 25 heroes, resulting in unacceptably slow training and degraded AI performance, even with thousands of GPUs (see Section“More heroes” in [24] for more details). Therefore, we need MOBA AI learning methods that deal with scalability-related issues caused by expanding the hero pool.</p>
<p>In this paper, we propose a learning paradigm for supporting full MOBA game-playing with deep reinforcement learning. Under the actor-learner pattern [12], we first build a distributed RL infrastruc-ture that generates training data in an off-policy manner. We then develop a unified actor-critic [20]network architecture to capture the playing mechanics and actions of different heroes. To deal with policy deviations caused by the diversity of game episodes, we apply off-policy adaption, following that of [38]. To manage the uncertain value of state-action in game, we introduce multi-head value estimation into MOBA by grouping reward items. Inspired by the idea of curriculum learning [1] for neural network, we design a curriculum for the multi-agent training in MOBA, in which we “start small” and gradually increase the difficulty of learning. Particularly, we start with fixed lineups to obtain teacher models, from which we distill policies [26], and finally we perform merged training. We leverage student-driven policy distillation [9] to transfer the knowledge from easy tasks to difficult ones. Lastly, an emerging problem with expanding the hero pool is drafting, a.k.a. hero selection,at the beginning of a MOBA game. The Minimax algorithm [18] for drafting used in existing work with a small-sized hero pool [2] is no longer computationally feasible. To handle this, we develop an efficient and effective drafting agent based on Monte-Carlo tree search (MCTS) [7].</p>
<p>Note that there still lacks a large-scale performance test of Game AI in the literature, due to the expensive nature of evaluating AI agents in real games, particularly for sophisticated video games. For example, AlphaStar Final [33] and OpenAI Five [2] were tested: 1) against professionals for 11matches and 8 matches, respectively; 2) against the public for 90 matches and for 7,257 matches,respectively (all levels of players can participate without an entry condition). To provide more statistically significant evaluations, we conduct a large-scale MOBA AI test. Specifically, we test using Honor of Kings, a popular and typical MOBA game, which has been widely used as a testbed for recent AI advances [36,38,37]. AI achieved a 95.2% win-rate over 42 matches against professionals,and a 97.7% win-rate against players of High King level3over 642,047 matches.</p>
<p>To sum up, our contributions are:</p>
<ul>
<li>We propose a novel MOBA AI learning paradigm towards playing full MOBA games with deep reinforcement learning.</li>
<li>We conduct the first large-scale performance test of MOBA AI agents. Extensive experiments show that our AI can defeat top esports players.</li>
</ul>
<h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><p>Our work belongs to system-level AI development for strategy video game playing, so we mainly discuss representative works along this line, covering RTS and MOBA games.</p>
<p><strong>General RTS games</strong> </p>
<p>StarCraft has been used as the testbed for Game AI research in RTS for many years. Methods adopted by existing studies include rule-based, supervised learning, reinforcement learning, and their combinations [23,34]. For rule-based methods, a representative is SAIDA,the champion of StarCraft AI Competition 2018 (see<a target="_blank" rel="noopener" href="https://github.com/TeamSAIDA/SAIDA).For">https://github.com/TeamSAIDA/SAIDA).For</a> learning-based methods, recently, AlphaStar combined supervised learning and multi-agent reinforcement learning and achieved the grandmaster level in playing StarCraft 2 [33]. Our value estimation (Section 3.2) shares similarity to AlphaStar’s by using invisible opponent’s information.</p>
<p><strong>MOBA games</strong></p>
<p>Recently, a macro strategy model, named Tencent HMS, was proposed for MOBA Game AI [36]. Specifically, HMS is a functional component for guiding where to go on the map during the game, without considering the action execution of agents, i.e., micro control or micro-management in esports, and is thus not a complete AI solution. The most relevant works are Tencent Solo [38] and OpenAI Five [2]. Ye et al. [38] performed a thorough and systematic study on the playing mechanics of different MOBA heroes. They developed a RL system that masters the micro control of agents in MOBA combats. However, only 1v1 solo games were studied without the much more sophisticated multi-agent 5v5 games. On the other hand, the similarities between this work and Ye et al. [38] include: the modeling of action heads (the value heads are different) and off-policy correction (adaption). In 2019, OpenAI introduced an AI for playing 5v5 games in Dota 2, called OpenAI Five, with the ability to defeat professional human players [2]. OpenAI Five is based on deep reinforcement learning via self-play. It trains using Proximal Policy Optimization (PPO) [28].The major difference between our work and OpenAI Five is that the goal of this paper is to develop AI programs towards playing full MOBA games. Hence, methodologically, we introduce a set of techniques of off-policy adaption, curriculum self-play learning, value estimation, and tree-search that addresses the scalability issue in training and playing a large pool of heroes. On the other hand, the similarities between this work and OpenAI Five include: the design of action space for modeling MOBA hero’s actions, the use of recurrent neural network like LSTM for handling partial observability, and the use of one model with shared weights to control all heroes.</p>
<h1 id="3-Learning-System"><a href="#3-Learning-System" class="headerlink" title="3. Learning System"></a>3. Learning System</h1><p>To address the complexity of MOBA game-playing, we use a combination of novel and existing learning techniques for neural network architecture, distributed system, reinforcement learning,multi-agent training, curriculum learning, and Monte-Carlo tree search. Although we use Honor of Kings as a case study, these proposed techniques are also applicable to other MOBA games, as the playing mechanics across MOBA games are similar.</p>
<h2 id="3-1-Architecture"><a href="#3-1-Architecture" class="headerlink" title="3.1 Architecture"></a>3.1 Architecture</h2><p>MOBA can be considered as a multi-agent Markov game with partial observations. Central to our AI is a policyπθ(at|st)represented by a deep neural network with parametersθ. It receives previous observations and actionsst= o1:t, a1:t−1from the game as inputs, and selects actionsatas outputs. Internally, observationsotare encoded via convolutions and fully-connected layers, then combined as vector representations, processed by a deep sequential network, and finally mapped to a probability distribution over actions. The overall architecture is shown in Fig. 1.</p>
<p>The architecture consists of general-purpose network components that model the raw complexity of MOBA games. To provide informative observations to agents, we develop multi-modal features,consisting of a comprehensive list of both scalar and spatial features. Scalar features are made up of observable units’ attributes, in-game statistics and invisible opponent information, e.g., health point (hp), skill cool down, gold, level, etc. Spatial features consist of convolution channels extracted from hero’s local-view map. To handle partial observability, we resort to LSTM [14] to maintain memories between steps. To help target selection, we use target attention [38,2], which treats the encodings after LSTM as query, and the stack of game unit encodings as attention keys. To eliminate unnecessary RL explorations, we design action mask, similar to [38]. To manage the combinatorial action space of MOBA, we develop hierarchical action heads and discretize each head. Specifically,AI predicts the output actions hierarchically: 1) what action to take, e.g., move, attack, skill releasing,etc; 2) who to target, e.g., a turret or an enemy hero or others; 3) how to act, e.g., a discretized direction to move.</p>
<h2 id="3-2-强化学习"><a href="#3-2-强化学习" class="headerlink" title="3.2 强化学习"></a>3.2 强化学习</h2><h2 id="3-3-多智能体训练"><a href="#3-3-多智能体训练" class="headerlink" title="3.3 多智能体训练"></a>3.3 多智能体训练</h2><h2 id="3-4-Learning-to-draft"><a href="#3-4-Learning-to-draft" class="headerlink" title="3.4 Learning to draft"></a>3.4 Learning to draft</h2><h2 id="3-5-Infrastructure"><a href="#3-5-Infrastructure" class="headerlink" title="3.5 Infrastructure"></a>3.5 Infrastructure</h2><h1 id="4-Evaluation"><a href="#4-Evaluation" class="headerlink" title="4. Evaluation"></a>4. Evaluation</h1><h2 id="4-1-Experimental-Setup"><a href="#4-1-Experimental-Setup" class="headerlink" title="4.1 Experimental Setup"></a>4.1 Experimental Setup</h2><h2 id="4-2-Experimental-Results"><a href="#4-2-Experimental-Results" class="headerlink" title="4.2 Experimental Results"></a>4.2 Experimental Results</h2><h3 id="4-2-1-AI-Performance"><a href="#4-2-1-AI-Performance" class="headerlink" title="4.2.1 AI Performance"></a>4.2.1 AI Performance</h3><h3 id="4-2-2-Training-Process"><a href="#4-2-2-Training-Process" class="headerlink" title="4.2.2 Training Process"></a>4.2.2 Training Process</h3><h3 id="4-2-3-Ablations"><a href="#4-2-3-Ablations" class="headerlink" title="4.2.3 Ablations"></a>4.2.3 Ablations</h3><h1 id="5-Conclusion-and-Future-Work"><a href="#5-Conclusion-and-Future-Work" class="headerlink" title="5. Conclusion and Future Work"></a>5. Conclusion and Future Work</h1><h1 id="6-Broader-Imapct"><a href="#6-Broader-Imapct" class="headerlink" title="6. Broader Imapct"></a>6. Broader Imapct</h1><h1 id="7-Funding-Disclosure"><a href="#7-Funding-Disclosure" class="headerlink" title="7. Funding Disclosure"></a>7. Funding Disclosure</h1><p>This work is supported by the Tencent AI Department and the Tencent TiMi Studio.</p>
<h1 id="8-Supplementary-Materials-补充材料"><a href="#8-Supplementary-Materials-补充材料" class="headerlink" title="8. Supplementary Materials/补充材料"></a>8. Supplementary Materials/补充材料</h1><h2 id="8-1-Infrastructure-Design"><a href="#8-1-Infrastructure-Design" class="headerlink" title="8.1 Infrastructure Design"></a>8.1 Infrastructure Design</h2><h2 id="8-2-Game-Environment-游戏环境"><a href="#8-2-Game-Environment-游戏环境" class="headerlink" title="8.2 Game Environment/游戏环境"></a>8.2 Game Environment/游戏环境</h2><h2 id="8-3-Hero-Pool-英雄池"><a href="#8-3-Hero-Pool-英雄池" class="headerlink" title="8.3 Hero Pool/英雄池"></a>8.3 Hero Pool/英雄池</h2><h2 id="8-4-Agent-Action-智能体动作"><a href="#8-4-Agent-Action-智能体动作" class="headerlink" title="8.4 Agent Action/智能体动作"></a>8.4 Agent Action/智能体动作</h2><h2 id="8-5-Reeward-Design-奖励设计"><a href="#8-5-Reeward-Design-奖励设计" class="headerlink" title="8.5 Reeward Design/奖励设计"></a>8.5 Reeward Design/奖励设计</h2><h2 id="8-6-Feature-Design-特征设计"><a href="#8-6-Feature-Design-特征设计" class="headerlink" title="8.6 Feature Design/特征设计"></a>8.6 Feature Design/特征设计</h2><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><div id="paginator"></div></div><div id="post-footer"><hr><a href="/post/3822e288/">← Prev メイドさんを誘惑するお嬢様 01</a><span style="color: #fe2"> | </span><a href="/post/f0c2be44/">わたしが恋人になれるわけないじゃん、ムリムリ！03 Next →</a><hr></div><div id="bottom-btn"><a id="to-index" href="#post-index" title="index">≡</a><a id="to-top" href="#post-title" title="to top">∧</a></div><div id="Valine"></div><script>new Valine({
 el: '#Valine'
 , appId: ''
 , appKey: ''
 , placeholder: '此条评论委托企鹅物流发送'
})</script></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://cdn.jsdelivr.net/gh/Sparklesyo/Collections/img/FNnex.png" alt="Logo"></a><h1 id="Dr"><a href="/"> Dr. Sparkle</a></h1><section id="total"><a id="total-archives" href="/archives"><span class="total-title">Archives Total:</span><span class="total-number">19</span></a><div id="total-tags"><span class="total-title">Tags:</span><span class="total-number">10</span></div><div id="total-categories"><span class="total-title">Categories:</span><span class="total-number">7</span></div></section></div><div id="aside-block"><h1>INDEX</h1><div id="post-index"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">基本信息</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Introduction"><span class="toc-number">3.</span> <span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Related-Work"><span class="toc-number">4.</span> <span class="toc-text">2. Related Work</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Learning-System"><span class="toc-number">5.</span> <span class="toc-text">3. Learning System</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Architecture"><span class="toc-number">5.1.</span> <span class="toc-text">3.1 Architecture</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">5.2.</span> <span class="toc-text">3.2 强化学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E8%AE%AD%E7%BB%83"><span class="toc-number">5.3.</span> <span class="toc-text">3.3 多智能体训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-Learning-to-draft"><span class="toc-number">5.4.</span> <span class="toc-text">3.4 Learning to draft</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-Infrastructure"><span class="toc-number">5.5.</span> <span class="toc-text">3.5 Infrastructure</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Evaluation"><span class="toc-number">6.</span> <span class="toc-text">4. Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-Experimental-Setup"><span class="toc-number">6.1.</span> <span class="toc-text">4.1 Experimental Setup</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-Experimental-Results"><span class="toc-number">6.2.</span> <span class="toc-text">4.2 Experimental Results</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-AI-Performance"><span class="toc-number">6.2.1.</span> <span class="toc-text">4.2.1 AI Performance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-Training-Process"><span class="toc-number">6.2.2.</span> <span class="toc-text">4.2.2 Training Process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3-Ablations"><span class="toc-number">6.2.3.</span> <span class="toc-text">4.2.3 Ablations</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Conclusion-and-Future-Work"><span class="toc-number">7.</span> <span class="toc-text">5. Conclusion and Future Work</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Broader-Imapct"><span class="toc-number">8.</span> <span class="toc-text">6. Broader Imapct</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Funding-Disclosure"><span class="toc-number">9.</span> <span class="toc-text">7. Funding Disclosure</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-Supplementary-Materials-%E8%A1%A5%E5%85%85%E6%9D%90%E6%96%99"><span class="toc-number">10.</span> <span class="toc-text">8. Supplementary Materials&#x2F;补充材料</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-Infrastructure-Design"><span class="toc-number">10.1.</span> <span class="toc-text">8.1 Infrastructure Design</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-Game-Environment-%E6%B8%B8%E6%88%8F%E7%8E%AF%E5%A2%83"><span class="toc-number">10.2.</span> <span class="toc-text">8.2 Game Environment&#x2F;游戏环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-Hero-Pool-%E8%8B%B1%E9%9B%84%E6%B1%A0"><span class="toc-number">10.3.</span> <span class="toc-text">8.3 Hero Pool&#x2F;英雄池</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-Agent-Action-%E6%99%BA%E8%83%BD%E4%BD%93%E5%8A%A8%E4%BD%9C"><span class="toc-number">10.4.</span> <span class="toc-text">8.4 Agent Action&#x2F;智能体动作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-Reeward-Design-%E5%A5%96%E5%8A%B1%E8%AE%BE%E8%AE%A1"><span class="toc-number">10.5.</span> <span class="toc-text">8.5 Reeward Design&#x2F;奖励设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-6-Feature-Design-%E7%89%B9%E5%BE%81%E8%AE%BE%E8%AE%A1"><span class="toc-number">10.6.</span> <span class="toc-text">8.6 Feature Design&#x2F;特征设计</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">11.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div><footer style="text-align: right"><nobr>published with&nbsp;<a target="_blank" rel="noopener" href="http://hexo.io">Hexo&nbsp;</a></nobr><wbr><nobr>Theme&nbsp;<a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknight&nbsp;</a></nobr><wbr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/arknights.js"></script><script src="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script></body></html>